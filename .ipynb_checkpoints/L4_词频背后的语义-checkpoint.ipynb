{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从词频到主题得分\n",
    "希望获得词或文档所想要表达的含义。  \n",
    "## TF-IDF向量及词形归并\n",
    "希望通过归一化方法处理词，避免TF-IDF的矩阵中词相当混乱，但归一化方法有时也会将相反含义的词放在一起，所以在处理上需要小心一点。  \n",
    "## 主题向量\n",
    "需要一种方法来从词的统计数据中提取一些额外的信息，称这些紧凑的意义向量为词-主题向量(word-topic vector)，称文档的意义向量为文档-主题向量(document-topic vector)。  \n",
    "通过两个词在相同文档中的共现频率来计算出两个词之间的同义性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个主题评分算法\n",
    "通过计算词和上下文在同一文档中的共现次数可以得到词的上下文的表示，进而通过一个词的上下文来理解这个词。  \n",
    "## 隐性语义分析\n",
    "latent semantic analysis,LSA， 也通常被称为隐性语义所以(latent semantic indexing, LSI)  \n",
    "一种分析TF-IDF矩阵的算法，通常被认为是一种降维技术，LSA减少了捕获文档含义所需要的维数。  \n",
    "一种揭示词组合的意义的算法。通过LSA可以将词的意义表示为向量，还可以将整篇文档的意义以向量表示。  \n",
    "与主成分分析(Principal Component Analysis, PCA)计算方法相同，当减少图像或其他数值表格而不是词袋向量或TF-IDF向量的维数时，就说是PCA。   \n",
    "与LSA相似的两个算法：\n",
    "1. 线性判别分析(Linear discriminant analysis, LDA)\n",
    "2. 隐性狄利克雷分布(latent Dirichlet allocation, LDiA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA分类器\n",
    "LDA是最直接也是最快速的降维和分类模型之一，是一种有监督算法，故需要对文档的类进行标注，但所需要但样本数要比其他算法少的多。  \n",
    "  \n",
    "简单实现版本的三个步骤：\n",
    "1. 计算某个类(如垃圾短消息类)中所有TF-IDF向量的平均位置(质心)。\n",
    "2. 计算不在该类(如非垃圾短消息类)中的所有TF-IDF向量的平均位置(质心)。\n",
    "3. 计算上述两个质心之间的向量差(即连接这两个向量的直线)。\n",
    "要利用该模型进行推理或推测，只需要判断新的TF-IDF向量是接近类内(垃圾类)而不是类外(非垃圾类)的质心。  \n",
    "得到类两个质心的向量差(直线距离)后，利用点积将每个TF-IDF向量投影到两质心的连线上，从而计算出得分。  \n",
    "LDA模型的参数很少，所以应该可以很好的泛化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDiA\n",
    "隐性狄利克雷分布，Latent Dirchlet Allocation，LDiA  \n",
    "可以用来捕捉词或文档语义的向量。  \n",
    "使用非线性计算法将词分组，通常会比线性方法(如LSA)的训练时间长很多，故在实际应用中并不常用，但它所创建的主题的统计数据有时更接近人类对词和主题的错觉，所以LDiA主题通常更易于向上级解释。  \n",
    "LDiA对一些单文档问题有用，如文档摘要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐性语义分析\n",
    "Latent Semantic Analysis， LSA  \n",
    "隐性语义分析来自于**奇异值分解(SVD)** 这项降维技术，奇异值分解会将一个矩阵分解成3个方阵，其中一个是对角矩阵。  \n",
    "SVD的一个应用是求逆矩阵，一个矩阵可以通过SVD分解成三个方阵，再对这些方阵求转置后再将它们相乘，就得到原始矩阵的逆矩阵。  \n",
    "利用SVD，LSA可以将TF-IDF词项-文档矩阵分解成为三个更简单的矩阵，类似于矩阵的因式分解，并在这三个矩阵相乘之前对它们进行截断处理(忽略一些行或列)，这将减少在向量空间模型需要处理的维数。  \n",
    "这些截断后的矩阵虽相乘后无法得到与原始TF-IDF矩阵完全一样的矩阵，但却给出了一个更好的矩阵。  \n",
    "文档的心表示包含了这些文档的新本质，即隐性语义；故SVD广泛用于压缩，因其可以捕捉数据集的本质，并忽略掉噪声。  \n",
    "当在自然语言处理中以这种方法使用SVD时，称其为隐性语义分析(LSA)。  \n",
    "LSA用于寻找对任意一组NLP向量进行最佳的线性变化，提供了另一条有用的信息，类似于TF-IDF的IDF部分，告诉我们向量中的哪些维度对文档语义很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
