{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码-解码结构\n",
    "encoder-decoder architecture  \n",
    "编码-解码结构的前半部分是序列编码器，该网络将序列(如自然语言文本)转换为较低维的表示形式，即为序列到序列模型的前半部分。  \n",
    "编码-解码结构的后半部分是序列解码器，序列解码器设计成将向量重新转换为人类可读的文本。  \n",
    "  \n",
    "当训练任何一个神经网络模型时，每个内部网络层都包含了所有需要通过训练解决的问题的信息。  \n",
    "该信息通常由一个固定维度的张量表示，该张量包含该层的权重或激活函数。  \n",
    "如果网络能够很好的概括所有信息，就可以确定会存在一个信息瓶颈-一个维度数量最少的层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码思想\n",
    "假设翻译模型，希望将字符序列或词序列映射到另一个字符或词序列。对于生成模型是不对的，因为在翻译任务中，输入序列和输出序列不一定具有相同的序列长度。  \n",
    "序列到序列网络架构，有时缩写为seq2seq，通过创建一个向量形式的输入表示来生成输出序列。  \n",
    "序列到序列网络由两个模块化的循环网络组成。  \n",
    "第一个部分为解码器，将输入文本转换为句向量，句向量有两部分，每一部分都是一个向量：编码器隐藏层的输出和输入样本LSTM的记忆状态，再将句向量作为第二个解码器的输入。  \n",
    "  \n",
    "在seq2seq结构中，训练阶段和推理阶段的处理是不同的：在训练期间，将初始文本传递给编码器，并将预测的文本作为输入传递给解码器。  \n",
    "我们让解码器网络学习，在给定初始状态和初始token的情况下，它就可以生成一系列词条。  \n",
    "解码器的第一个输入是初始化词条，第二个输入应是第一个预期的或要被预测的词条，它会反过来促使网络生成第二个预期的词条。  \n",
    "  \n",
    "解码器在推理阶段，使用初始词条生成第一个元素，然后使用第一个元素作为下一个时刻解码器的输入，以生成下一个元素以此类推，不断重复直到序列元素的最大数量。  \n",
    "  \n",
    "通过这种端到端(end-to-end)训练，解码器将把一个句向量转换为对初始输入序列的全解码回复。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码器\n",
    "自编码器虽与encoder-decoder具有相同结构，但它们是为不同任务而训练的。自编码器被训练来寻找输入数据的向量表示，这样输入就能由网络的解码器以最小的误差重构。  \n",
    "编码器和解码器互为伪逆过程(pseudo-inverses)，该网络的目的是找到输入数据的稠密向量表示，从而允许解码器以最小的误差重构它。  \n",
    "在训练阶段，输入数据和预期输出是相同的，因此，若目标是找到数据的稠密向量表示，而不是为语言翻译生成向量或为给定的问题寻找答复，那自编码器是一个不错的选择。  \n",
    "  \n",
    "## 变分自编码器\n",
    "自编码器的一种变体，用于训练一个类似于编码-解码架构的生成器。  \n",
    "变分自编码器产生的压缩向量不仅是输入的准确表示，而且是满足高斯分布的。  \n",
    "通过随机选择一个种子向量并将其输入自编码器后半部分的解码器中，就可以更容易的生成一个全新的输出。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列到序列对话\n",
    "翻译和对话任务都需要模型将一个序列映射到另一个序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 组装一个序列到序列的流水线\n",
    "## 为序列到序列训练准备数据集\n",
    "需要将输入数据填充至固定长度。需要使用填充词条扩展输入序列，使其与最长的输入序列匹配。  \n",
    "在序列到序列网络的情况下，还需要准备目标数据并填充它以匹配最长的目标序列。  \n",
    "记住，输入数据和目标数据长度不需相等。  \n",
    "  \n",
    "在目标序列上，需要准备两个版本的目标序列来进行训练：一个以初始词条开始(将初始词条作为解码器输入)，另一个不以初始词条开始(损失函数将对目标序列的精准性进行评分)。  \n",
    "序列到序列模型的每个训练样本都是一个三元组：初始输入、预期输出(以初始词条为前置项)和预期输出(没有初始词条)。  \n",
    "  \n",
    "序列到序列网络由两个网络组成：  \n",
    "1. 编码器：将生成句向量\n",
    "2. 解码器：将句向量作为它的初始状态传递。  \n",
    "  \n",
    "将初始化状态和初始词条作为解码器网络的输入，网络会生成输出第一个的序列元素(如字符或词向量)。再根据更新后的状态和预期序列中的下一个元素预测下面的每一个元素。这个过程将持续进行，直到生成一个终止词条或达到元素的最大数量。解码器生成的所有序列元素将组成预测的输出。  \n",
    "  \n",
    "## Keras中的序列到序列模型\n",
    "在训练阶段，将端到端的同时训练编码器和解码器网络，每个样本由三个数据所组成：一个训练编码器的输入序列、一个解码器输入序列和一个解码器输出序列。训练编码器输入序列可以是一个用户的问题，而解码器输入序列是未来机器人的预期回复。  \n",
    "\n",
    "## 序列编码器\n",
    "编码器的唯一目的是创建句向量，然后将其作为解码器网络的初始状态。  \n",
    "**由于句向量并没有一个目标进行学习，因此无法单独训练一个编码器，反向误差将训练编码器创建一个合适的句向量，而反向传播的值来自之后下游解码器产生的误差**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras中的思想编码\n",
    "encoder_inputs = Input(shape=(None, input_vocab_size))  \n",
    "encoder = LSTM(num_neurons, return_state=True)  # LSTM层的return_state参数需要设置为True才能返回内部状态\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = (state_h, state_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state_c是记忆单元当前的状态，state_h和state_c将构成句向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列解码器\n",
    "使用样本三元组的第二部分和第三部分。  \n",
    "解码器具有标准的一个接着一个词条的输入和输出，它们几乎是一样的，但相差一个时刻。  \n",
    "我们希望解码器在给定状态的情况下，学习重新产生给定输入序列的词条，而该状态由传入编码器的三元组第一部分产生。  \n",
    "  \n",
    "要计算训练阶段的误差，需要将LSTM层的输出传递到另一个稠密层。稠密层的神经元数量将等于所有可能输出的词条数量。稠密层将使用softmax激活函数覆盖这些词条。故在每个时刻，网络将为它认为最有可能的下一个序列元素在所有可能的词条上提供一个概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, output_vocab_size))\n",
    "decoder_lstm = LSTM(num_neurons, return_sequences=True, return_state=True)  # 设置LSTM层，类似于编码器，但附加来一个return_sequences参数\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)  # 将最后一个编码器状态赋值给initial_state\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')  # 将所有可能字符映射到softmax的输出层\n",
    "decoder_outputs = decoder_dense(decoder_outputs)  # 将LSTM层的输出传递给softmax层\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组装一个序列到序列网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个序列到序列模型\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成输出序列\n",
    "encdoer_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增强\n",
    "有两种增强序列到序列模型的方法，可以提高模型的精确率和可扩展性。\n",
    "## 使用装桶法降低训练复杂度\n",
    "输入序列可以有不同的长度，使得短序列的训练数据添加来大量的填充词条，过多的填充词条会使得计算成本上升，特别是当大多数序列都很短时，只有少数序列接近最大词条长度时。  \n",
    "  \n",
    "装桶法可以减少计算量，可以按照长度对序列排序，并在不同的批处理期间使用不同的序列长度。  \n",
    "将输入序列分配到不同长度的桶中，例如长度在5～10个词条之间的所有序列放在一个桶中，然后训练该批次时使用这个序列的桶，例如：先训练5～10个词条之间的所有序列，然后训练10～15个词条之间的所有序列。  \n",
    "  \n",
    "## 注意力机制\n",
    "attention mechanism  \n",
    "告诉解码器应该注意输入序列中的哪些部分。  \n",
    "注意力机制通过选择与输出相关的输入部分，允许输入和输出之间直接连接，但并不意味输入和输出序列的词条要对齐。  \n",
    "注意力机制，在给定解码器时刻时，解码器都会接收一个额外的具有每个时刻的输入，表示要注意的输入序列中的一个或多个词条。编码器中所有序列的重要成都将由解码器各个时刻的加权平均值表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
